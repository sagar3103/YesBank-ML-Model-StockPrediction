{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1_RNMxyigj1t6tV3jWuB69sCerTue4L1Q","timestamp":1750403015012}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - Predicting Yes Bank's Monthly Stock Closing Price\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["#### **Project Type**    - Regression\n","#### **Contribution**    - Individual\n","#### **Name** - Sagar Zujam\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["###**This project involves predicting the monthly closing stock price of Yes Bank using historical stock data. Yes Bank, one of India's prominent private banks, experienced major fluctuations in its stock price, particularly post-2018 due to financial instability and fraud cases. The dataset includes monthly stock data such as open, high, low, and close prices, which allows for regression modeling to forecast the stock's closing price. The goal is to understand trends, perform deep EDA, and apply machine learning techniques to accurately predict the closing price, thereby providing valuable insights into stock behavior.**"],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["###**To build a predictive model that estimates Yes Bank's monthly stock closing price using historical stock price data, and to analyze how trends and volatility can be captured using time series and regression techniques.**"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as stats\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"data_YesBank_StockPrices.csv\")"],"metadata":{"id":"EyKgRMd9hjVq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","print(\"\\nFirst 5 Rows:\")\n","print(df.head())"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","print(\"\\nNumber of Rows:\", df.shape[0])\n","print(\"Number of Columns:\", df.shape[1])"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","print(\"\\nData Types:\")\n","print(df.dtypes)"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(\"\\nChecking for duplicate records:\")\n","print(df.duplicated().sum())"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","print(\"\\nNull Values:\")\n","print(df.isnull().sum())"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing Missing Values with Heatmap\n","plt.figure(figsize=(10, 2))\n","sns.heatmap(df.isnull(), cbar=False, cmap=\"Reds\", yticklabels=False)\n","plt.title(\"Missing Value Heatmap\")\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["The dataset consists of monthly stock price data for Yes Bank spanning from July 2005 to November 2020, with 185 observations and 5 key columns:\n","\n","1. Date – Contains month and year of the record.\n","2. Open – Stock price at the beginning of the month.\n","3. High – Highest stock price during the month.\n","4. Low – Lowest stock price during the month.\n","5. Close – Stock price at the end of the month (Target variable)"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["### Convert Date to datetime and sort"],"metadata":{"id":"GluWU2KTHzZC"}},{"cell_type":"code","source":["print(\"\\nConverting 'Date' to datetime format...\")\n","df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n","df = df.sort_values('Date')\n","df.reset_index(drop=True, inplace=True)"],"metadata":{"id":"X7TwOoURIL5i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####1. Add additional time-based features (month and year) for better analysis"],"metadata":{"id":"rNgTg6JyIXEN"}},{"cell_type":"code","source":["print(\"\\nAdding 'Month' and 'Year' columns...\")\n","df['Month'] = df['Date'].dt.month"],"metadata":{"id":"o1iCdqLrIhEm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####2. Since only day and month provided, we simulate year sequence manually"],"metadata":{"id":"-aN2drcvI4Zg"}},{"cell_type":"code","source":["print(\"\\nCreating synthetic 'Year' based on position...\")\n","df['Year'] = [i//12 + 2005 for i in range(len(df))]  # Assume data starts from July 2005"],"metadata":{"id":"SZ-hIgJRJA2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####3. Rearrange columnss"],"metadata":{"id":"70hNPsaYJM5v"}},{"cell_type":"code","source":["df = df[['Date', 'Year', 'Month', 'Open', 'High', 'Low', 'Close']]\n","\n","# Show final structure\n","print(\"\\nUpdated DataFrame Head:\")\n","print(df.head())"],"metadata":{"id":"eR_p-fxYJUDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","print(\"\\nColumn Names:\")\n","print(df.columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","print(\"\\nDescriptive Statistics of Dataset:\")\n","print(df.describe())\n","\n","# Data Types confirmation\n","print(\"\\nData Types after processing:\")\n","print(df.dtypes)"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["1. Strong correlation observed between High and Close, and Low and Close.\n","2. Open, High, and Low may serve as good predictors for the Close value.\n","3. No duplicate records and all dates are unique after transformation.\n","4. Date was only used for temporal ordering and visualization; it was dropped before modeling due to its non-numeric format."],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","print(\"\\nAre Dates Unique?:\", df['Date'].is_unique)\n","\n","# Correlation Check\n","print(\"\\nCorrelation Matrix:\")\n","print(df.corr())\n","\n","# Heatmap for correlation\n","plt.figure(figsize=(8,6))\n","sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n","plt.title('Correlation Matrix')\n","plt.show()"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Checking for missing values again\n","print(\"\\nChecking for missing values before imputation:\")\n","print(df.isnull().sum())\n","\n","# Check for outliers using IQR method\n","def detect_outliers(column):\n","    Q1 = df[column].quantile(0.25)\n","    Q3 = df[column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    outliers = df[(df[column] < lower) | (df[column] > upper)]\n","    return outliers\n","\n","outlier_summary = {}\n","for col in ['Open', 'High', 'Low', 'Close']:\n","    outliers = detect_outliers(col)\n","    outlier_summary[col] = len(outliers)\n","    print(f\"Outliers in {col}: {len(outliers)}\")\n","\n","# Optionally handle outliers: Capping method used here (optional, for demonstration)\n","for col in ['Open', 'High', 'Low', 'Close']:\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    df[col] = np.where(df[col] < lower, lower, df[col])\n","    df[col] = np.where(df[col] > upper, upper, df[col])\n","\n","print(\"\\nShape after outlier capping:\", df.shape)\n","\n","# Check again for missing values\n","print(\"\\nMissing values after handling:\")\n","print(df.isnull().sum())\n","\n","# Final check of dataset\n","df.reset_index(drop=True, inplace=True)\n","print(\"\\nData Wrangling complete.\")"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["1. **Missing Values:** No missing values were found in the dataset.\n","\n","2. **Outlier Detection:** Used the IQR method to detect outliers in Open, High, Low, and Close.\n","\n","3. **Outlier Handling:** Applied capping to limit the influence of extreme values.\n","\n","4. **Outcome:** Data became clean, with reduced skewness, making it suitable for modeling."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1: Univariate Analysis: Distribution of each stock price component"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","features = ['Open', 'High', 'Low', 'Close']\n","for feature in features:\n","    plt.figure(figsize=(8,4))\n","    sns.histplot(df[feature], kde=True, bins=30, color='skyblue')\n","    plt.title(f'Distribution of {feature} Price')\n","    plt.xlabel(f'{feature} Price')\n","    plt.ylabel('Frequency')\n","    plt.grid(True)\n","    plt.show()\n","\n","    print(f\"Why this chart: To observe the distribution and skewness of {feature} prices.\")\n","    print(\"Insights: Outliers and skewed patterns observed due to major price fluctuations over years.\")\n","    print(\"Business Impact: Helps in normalization decisions for modeling, detecting non-stationary behavior.\")\n"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["To observe the distribution and skewness of {feature} prices."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Insights: Outliers and skewed patterns observed due to major price fluctuations over years."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Business Impact: Helps in normalization decisions for modeling, detecting non-stationary behavior.\n","\n","Yes Bank’s stock showed abnormal peaks and drops around 2018–2019, reflecting periods of financial instability and fraud.\n","By identifying these trends early through visualizations, we were able to design a more robust prediction pipeline, aiding better financial decision-making and reducing risk."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2 Bivariate Analysis: Relation of Open, High, Low with Close"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","for feature in ['Open', 'High', 'Low']:\n","    plt.figure(figsize=(6,4))\n","    sns.scatterplot(x=df[feature], y=df['Close'])\n","    plt.title(f'{feature} vs Close')\n","    plt.xlabel(feature)\n","    plt.ylabel('Close')\n","    plt.grid(True)\n","    plt.show()\n","\n","    print(f\"Why this chart: To analyze the relationship between {feature} and Close price.\")\n","    print(\"Insights: Strong linear relationship evident, especially for High and Low prices.\")\n","    print(\"Business Impact: These features are valuable predictors for stock closing price.\")"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["To analyze the relationship between {feature} and Close price."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Insights: Strong linear relationship evident, especially for High and Low prices."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3: Multivariate Analysis: Time-based trends"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","plt.figure(figsize=(15,5))\n","sns.lineplot(data=df, x='Year', y='Close', label='Close')\n","sns.lineplot(data=df, x='Year', y='High', label='High')\n","sns.lineplot(data=df, x='Year', y='Low', label='Low')\n","plt.title('Trend of Stock Prices Over Time')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","print(\"Why this chart: To understand stock price evolution over the years.\")\n","print(\"Insights: Significant spike and drop observed around 2018-2019.\")\n","print(\"Business Impact: Helps explain volatility periods and link them to external events like financial fraud.\")\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["To understand stock price evolution over the years."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Insights: Significant spike and drop observed around 2018-2019."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Business Impact: Helps explain volatility periods and link them to external events like financial fraud.\n","\n","1. The sharp decline after 2018 reflects real-world financial fraud at Yes Bank.\n","\n","2. This affects investor trust and must be accounted for in modeling to avoid misleading projections.\n","\n","These trends guide decision-makers in assessing when and why drastic price changes occurred, directly impacting business strategies."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Hypothesis 1: Is the mean 'Close' price before 2018 significantly different from after 2018?"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Hypothesis 1: Is the mean 'Close' price before 2018 significantly different from after 2018?\n","before_2018 = df[df['Year'] < 2018]['Close']\n","after_2018 = df[df['Year'] >= 2018]['Close']\n","\n","# Perform t-test\n","t_stat, p_value = stats.ttest_ind(before_2018, after_2018, equal_var=False)\n","print(\"\\nHypothesis Test 1: Mean comparison of Close price before and after 2018\")\n","print(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")\n","\n","if p_value < 0.05:\n","    print(\"Conclusion: Statistically significant difference in Close prices before and after 2018.\")\n","else:\n","    print(\"Conclusion: No significant difference in Close prices before and after 2018.\")"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Independent Two-Sample t-test (Welch’s t-test):\n","\n","1. stats.ttest_ind() is used to compare the means of two independent samples (in this case, Close prices before and after 2018).\n","\n","2. The parameter equal_var=False indicates we do not assume equal population variance, which makes it a Welch's t-test, a more robust version of the t-test.\n","\n"],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","1. You're comparing average values of a numeric variable across two groups (e.g., pre-2018 vs. post-2018).\n","\n","2. Sample sizes or variances are unequal."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Answer Here.\n","  Is there a significant correlation between 'High' and 'Close' prices?"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","correlation, pval = stats.pearsonr(df['High'], df['Close'])\n","print(\"\\nHypothesis Test 2: Correlation between High and Close\")\n","print(f\"Correlation Coefficient: {correlation:.4f}, P-Value: {pval:.4f}\")\n","\n","if pval < 0.05:\n","    print(\"Conclusion: Strong statistically significant correlation between High and Close.\")\n","else:\n","    print(\"Conclusion: No statistically significant correlation.\")"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Pearson Correlation Coefficient Test.\n","\n","1. stats.pearsonr() computes the linear correlation coefficient between two continuous variables — in this case, High and Close prices.\n","\n","2. It also returns a p-value, which tests the null hypothesis that the correlation is zero (i.e., no linear relationship)."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","This test is appropriate because:\n","\n","1. Both High and Close are continuous and approximately normally distributed (as confirmed in univariate analysis).\n","\n","2. Measuring the strength and significance of a linear relationship."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Answer Here. Is there a significant difference in average Close prices across months?"],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Hypothesis 3: Is there a significant difference in average Close prices across months?\n","from scipy.stats import f_oneway\n","\n","# Group Close prices by month\n","monthly_groups = [group[\"Close\"].values for name, group in df.groupby(\"Month\")]\n","\n","# Perform One-Way ANOVA\n","f_stat, p_val = f_oneway(*monthly_groups)\n","\n","print(\"\\nHypothesis Test 3: Difference in average Close prices across months (One-Way ANOVA)\")\n","print(f\"F-Statistic: {f_stat:.4f}, P-Value: {p_val:.4f}\")\n","\n","if p_val < 0.05:\n","    print(\"Conclusion: Significant variation in Close prices across different months.\")\n","else:\n","    print(\"Conclusion: No significant variation in Close prices across different months.\")\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","One-Way ANOVA (Analysis of Variance)\n","\n","1. **Test Used:** f_oneway (One-Way ANOVA)\n","\n","2. **Null Hypothesis:** All monthly means of Close prices are equal.\n","\n","3. **Alternative Hypothesis:** At least one month's mean Close price is different.\n","\n","4. **Reason:** Appropriate for comparing means across more than two independent groups (12 months here)."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","1. The test compares mean Close prices across multiple independent groups (in this case, the 12 months).\n","\n","2. We grouped the Close prices by Month and used scipy.stats.f_oneway() to check if at least one month's mean is significantly different from others."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"markdown","source":["Answer:\n","\n","Yes,\n","1. The target variable Close and predictors like Open, High, Low are already in continuous, linear scale.\n","\n","2. A visual inspection of their distribution showed mild skewness, but not extreme—hence transformation was not strictly necessary.\n","\n","3. Instead, we handled skewness and outliers using IQR-based capping, which stabilized the data.\n","\n","4. The StandardScaler was used for scaling to ensure equal contribution from all variables."],"metadata":{"id":"M-16xK8cam1q"}},{"cell_type":"markdown","source":["### 2. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["Answer:\n","\n","used StandardScaler from sklearn.preprocessing.\n","1. StandardScaler standardizes features by removing the mean and scaling to unit variance.\n","\n","2. This means each feature will have a mean of 0 and standard deviation of 1, which helps models like Linear Regression and Random Forest perform better.\n","\n","3. Scaling ensures all features are on the same scale, preventing bias toward higher magnitude variables like High or Low prices."],"metadata":{"id":"fTbKnHhcZuX-"}},{"cell_type":"markdown","source":["### 3. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Since the dataset is small, focused, and well-engineered, dimensionality reduction techniques are not required for this regression task."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"markdown","source":["### 4. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","\n","# Drop 'Date' since it's not a numeric feature\n","df_model = df.drop(['Date'], axis=1)\n","\n","# Define X and y\n","X = df_model.drop('Close', axis=1)\n","y = df_model['Close']\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nFeature Engineering and Preprocessing complete.\")\n","print(\"Training Feature Shape:\", X_train.shape)\n","print(\"Testing Feature Shape:\", X_test.shape)"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["Answer Here.\n","I used an 80:20 train-test split ratio.\n","\n","1. 80% of the data was used for training the machine learning models to learn the patterns.\n","\n","2. 20% of the data was kept for testing to evaluate how well the model generalizes on unseen data.\n","\n","3. This is a standard industry practice that provides a good balance between training and validation while avoiding overfitting or underfitting.\n","\n","4. The dataset has a moderate size (185 rows), so 80:20 offers enough training samples while keeping a meaningful test set."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1: Linear Regression"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 : Linear Regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","model1 = LinearRegression()\n","model1.fit(X_train, y_train)\n","y_pred1 = model1.predict(X_test)\n","\n","print(\"\\nModel 1 - Linear Regression\")\n","print(\"MAE:\", mean_absolute_error(y_test, y_pred1))\n","print(\"MSE:\", mean_squared_error(y_test, y_pred1))\n","print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred1)))\n","print(\"R2 Score:\", r2_score(y_test, y_pred1))"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Linear Regression Metrics\n","lr_metrics = {\n","    'MAE': 5.0563,\n","    'MSE': 71.5326,\n","    'RMSE': 8.4577,\n","    'R2': 0.9914\n","}\n","\n","# Plotting\n","plt.figure(figsize=(8, 5))\n","plt.bar(lr_metrics.keys(), lr_metrics.values(), color='skyblue')\n","plt.title('Linear Regression Evaluation Metrics')\n","plt.ylabel('Score')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","# Linear Regression Model\n","lr = LinearRegression()\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {\n","    'fit_intercept': [True, False]\n","}\n","\n","# GridSearchCV\n","grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='r2')\n","grid_search.fit(X_train, y_train)\n","\n","# Best Estimator\n","best_lr = grid_search.best_estimator_\n","print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","I used GridSearchCV as the hyperparameter optimization technique.\n","\n","1. Exhaustive Search: It evaluates all possible combinations of the given hyperparameter values, ensuring the best set is selected.\n","\n","2. Simplicity: Easy to implement and interpret, especially when the parameter space is small (as in Linear Regression).\n","\n","3. Cross-validation: It uses cross-validation internally, which helps in reducing overfitting and gives a more generalized model performance.\n","\n","4. Best suited: For models with fewer hyperparameters like Linear Regression, GridSearchCV is efficient and effective."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","There was no significant improvement in metrics after tuning because Linear Regression has very limited hyperparameters, and the dataset was already well-scaled and clean. The model was already performing optimally"],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2: RandomForestRegressor"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","model2 = RandomForestRegressor(random_state=42)\n","model2.fit(X_train, y_train)\n","y_pred2 = model2.predict(X_test)\n","\n","print(\"\\nModel 2 - Random Forest Regressor\")\n","print(\"MAE:\", mean_absolute_error(y_test, y_pred2))\n","print(\"MSE:\", mean_squared_error(y_test, y_pred2))\n","print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred2)))\n","print(\"R2 Score:\", r2_score(y_test, y_pred2))"],"metadata":{"id":"jDQGGsZNhDnS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","# Initialize the model\n","rf = RandomForestRegressor(random_state=42)\n","\n","# Define parameter grid for tuning\n","param_grid_rf = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Define the model\n","rf_model = RandomForestRegressor(random_state=42)\n","\n","# Define hyperparameter grid\n","param_dist = {\n","    'n_estimators': [50, 100, 150, 200],\n","    'max_depth': [5, 10, 15, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# RandomizedSearchCV\n","random_search = RandomizedSearchCV(\n","    estimator=rf_model,\n","    param_distributions=param_dist,\n","    n_iter=10,  # Number of combinations to try\n","    cv=5,\n","    scoring='neg_mean_squared_error',\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# Fit on training data\n","random_search.fit(X_train, y_train)\n","\n","# Best model from RandomizedSearch\n","best_rf_model = random_search.best_estimator_\n","\n","# Predict on test set\n","rf_y_pred = best_rf_model.predict(X_test)\n","\n","# Evaluation\n","rf_mae = mean_absolute_error(y_test, rf_y_pred)\n","rf_mse = mean_squared_error(y_test, rf_y_pred)\n","rf_rmse = np.sqrt(rf_mse)\n","rf_r2 = r2_score(y_test, rf_y_pred)\n","\n","# Print evaluation\n","print(\"Random Forest with RandomizedSearchCV:\")\n","print(f\"MAE: {rf_mae}\")\n","print(f\"MSE: {rf_mse}\")\n","print(f\"RMSE: {rf_rmse}\")\n","print(f\"R² Score: {rf_r2}\")\n"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","I have used RandomizedSearchCV for hyperparameter optimization of the Random Forest Regressor. This technique was chosen because it is computationally more efficient than GridSearchCV, especially when the hyperparameter space is large. It randomly selects a fixed number of parameter combinations from the specified grid, reducing the search time while still achieving good performance. It also helps prevent overfitting by using cross-validation."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Yes, after applying RandomizedSearchCV, the model's performance improved compared to the default Random Forest Regressor.\n","\n","MAE reduced by ~25%, indicating more accurate point predictions.\n","\n","RMSE and MSE also dropped significantly, showing fewer extreme errors.\n","\n","R² Score increased from 0.9788 to 0.9873, reflecting better overall model fit.\n","\n"],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","For the Random Forest Regressor, the evaluation metrics used were:\n","\n","1. MAE (Mean Absolute Error): Indicates the average error in predicted prices. In business terms, it reflects the average monthly deviation between actual and predicted stock prices, helping stakeholders understand typical forecasting errors.\n","\n","2. RMSE (Root Mean Square Error): Penalizes larger errors more than MAE, making it useful to identify high-risk months where stock behavior was volatile. This helps risk management teams plan better.\n","\n","3. R² Score: Shows how well the model explains the variance in stock prices. A high R² value indicates strong model performance, making it reliable for forecasting stock movements.\n","\n","**Business Impact**: These metrics help financial analysts trust the model’s predictions, plan better investments, and understand the consistency and reliability of the forecasts."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3: XGBRegressor"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Instantiate and fit the model\n","xgb_model = XGBRegressor(random_state=42)\n","xgb_model.fit(X_train, y_train)\n","\n","# Predict\n","xgb_preds = xgb_model.predict(X_test)\n","\n","# Evaluate\n","xgb_mae = mean_absolute_error(y_test, xgb_preds)\n","xgb_mse = mean_squared_error(y_test, xgb_preds)\n","xgb_rmse = np.sqrt(xgb_mse)\n","xgb_r2 = r2_score(y_test, xgb_preds)\n","\n","print(\"XGBoost Regressor Performance:\")\n","print(f\"MAE: {xgb_mae}\")\n","print(f\"MSE: {xgb_mse}\")\n","print(f\"RMSE: {xgb_rmse}\")\n","print(f\"R2 Score: {xgb_r2}\")\n"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# ----- Linear Regression -----\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","lr_pred = lr_model.predict(X_test)\n","\n","lr_mae = mean_absolute_error(y_test, lr_pred)\n","lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n","lr_r2 = r2_score(y_test, lr_pred)\n","\n","# ----- Random Forest Regressor -----\n","rf_model = RandomForestRegressor(random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_pred = rf_model.predict(X_test)\n","\n","rf_mae = mean_absolute_error(y_test, rf_pred)\n","rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n","rf_r2 = r2_score(y_test, rf_pred)\n","\n","# ----- XGBoost Regressor -----\n","xgb_model = XGBRegressor(random_state=42, verbosity=0)\n","xgb_model.fit(X_train, y_train)\n","xgb_pred = xgb_model.predict(X_test)\n","\n","xgb_mae = mean_absolute_error(y_test, xgb_pred)\n","xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n","xgb_r2 = r2_score(y_test, xgb_pred)\n","\n","# ----- Visualizing Evaluation Metric Score Chart -----\n","models = ['Linear Regression', 'Random Forest', 'XGBoost']\n","mae_scores = [lr_mae, rf_mae, xgb_mae]\n","rmse_scores = [lr_rmse, rf_rmse, xgb_rmse]\n","r2_scores = [lr_r2, rf_r2, xgb_r2]\n","\n","plt.figure(figsize=(15, 4))\n","\n","# MAE\n","plt.subplot(1, 3, 1)\n","plt.bar(models, mae_scores, color='orange')\n","plt.title('MAE Comparison')\n","plt.ylabel('MAE')\n","\n","# RMSE\n","plt.subplot(1, 3, 2)\n","plt.bar(models, rmse_scores, color='green')\n","plt.title('RMSE Comparison')\n","plt.ylabel('RMSE')\n","\n","# R² Score\n","plt.subplot(1, 3, 3)\n","plt.bar(models, r2_scores, color='skyblue')\n","plt.title('R² Score Comparison')\n","plt.ylabel('R² Score')\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"aHHVq2Bnt5BT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# Define parameter grid\n","xgb_param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","    'max_depth': [3, 5, 7, 10],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0]\n","}\n","\n","# Create the model\n","xgb_base = XGBRegressor(random_state=42)\n","\n","# RandomizedSearchCV\n","xgb_random_search = RandomizedSearchCV(\n","    estimator=xgb_base,\n","    param_distributions=xgb_param_grid,\n","    n_iter=20,\n","    cv=5,\n","    scoring='neg_mean_squared_error',\n","    random_state=42,\n","    n_jobs=-1,\n","    verbose=1\n",")\n","\n","xgb_random_search.fit(X_train, y_train)\n","\n","# Best model\n","best_xgb_model = xgb_random_search.best_estimator_\n","\n","# Predictions\n","xgb_best_preds = best_xgb_model.predict(X_test)\n","\n","# Evaluation\n","xgb_best_mae = mean_absolute_error(y_test, xgb_best_preds)\n","xgb_best_mse = mean_squared_error(y_test, xgb_best_preds)\n","xgb_best_rmse = np.sqrt(xgb_best_mse)\n","xgb_best_r2 = r2_score(y_test, xgb_best_preds)\n","\n","print(\"Tuned XGBoost Regressor Performance:\")\n","print(f\"MAE: {xgb_best_mae}\")\n","print(f\"MSE: {xgb_best_mse}\")\n","print(f\"RMSE: {xgb_best_rmse}\")\n","print(f\"R2 Score: {xgb_best_r2}\")\n"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","I used RandomizedSearchCV for hyperparameter optimization. This technique is more efficient than GridSearchCV when dealing with a large hyperparameter space. It samples a fixed number of parameter settings from the specified distributions, saving time and computational resources while still providing good results. It is ideal when we want a quicker yet effective tuning of model performance."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Yes, after applying RandomizedSearchCV to the XGBoost model, I observed a noticeable improvement in performance metrics:\n","\n","1. MAE reduced from 4.89 to 4.53\n","\n","2. RMSE reduced from 7.92 to 7.38\n","\n","3. R2 Score improved from 0.9921 to 0.9932"],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","Evaluation Metrics Used:\n","\n","1. **MAE** gives a clear interpretation of the average magnitude of prediction errors in real business units (rupees in this case), helping to understand the expected deviation from actual stock prices.\n","\n","2. **RMSE** penalizes larger errors more than MAE, making it suitable for stock prices where large deviations could significantly impact investment decisions.\n","\n","3. **R² Score** indicates how well the independent features explain the variation in the closing stock price. A higher R² directly relates to better model accuracy, which improves investor confidence and decision-making.\n","\n","These metrics collectively ensure the model's precision, reliability, and generalizability, which are essential when making financial predictions where every rupee matters."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["Answer Here.\n","\n"," **XGBoost Regressor** with RandomizedSearchCV (Tuned Version)\n","\n","1. Achieved the lowest MAE and RMSE among all models.\n","\n","2. Gave the highest R² Score (0.9932), indicating excellent prediction accuracy.\n","\n","3. XGBoost also handles non-linearity, outliers, and feature interactions better than traditional models like Linear Regression or Random Forest.\n","\n","4. Hyperparameter tuning via RandomizedSearchCV further improved its performance without overfitting."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","1. XGBoost is an ensemble-based gradient boosting algorithm that builds multiple decision trees in sequence.\n","\n","2. It optimizes performance by focusing on errors made by previous models, improving overall accuracy.\n","\n","3. It has built-in regularization, making it less prone to overfitting.\n","\n","Understanding feature contributions helps stakeholders and investors trust the model and make informed decisions. It also aids in identifying which stock attributes drive prices the most, leading to better investment strategies."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File\n","import joblib\n","\n","# Save the best model\n","filename = 'best_xgb_model.pkl'\n","joblib.dump(best_xgb_model, filename)\n","print(f\"\\nModel saved as {filename}\")\n"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data.\n","loaded_model = joblib.load(filename)\n","sample_prediction = loaded_model.predict(X_test[:5])\n","print(\"\\nSample Predictions from loaded model:\", sample_prediction)"],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["The Linear Regression outperformed Random Forest Regressor in all key evaluation metrics, indicating its robustness\n","in capturing the non-linear relationships in Yes Bank's stock data. The model was saved and validated on sample data\n","successfully, making it deployment-ready. Future work may include time-series-specific models like ARIMA or LSTM for\n","finer trend prediction."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"code","source":["print(\"\\nModel creation, evaluation, and deployment preparation complete.\")"],"metadata":{"id":"-AKmJDExKROs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}